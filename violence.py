# -*- coding: utf-8 -*-
"""Violence.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hxu573eBL513jvP9pBh7zxfDTCaLq3vU
"""


# pip install mediapipe opencv-contrib-python
import os
import cv2
import mediapipe as mp
import numpy as np
from sklearn.model_selection import train_test_split
# !pip install keras==2.15.0
# !pip install tensorflow==2.15.0
import tensorflow as tf
from tensorflow.keras import layers


# Load the model from the .h5 file
from tensorflow.keras.models import load_model

# Load the previously saved model
loaded_model = load_model('model.h5')

# Verify that the model is loaded
print("Model loaded successfully!")

import tensorflow as tf
import cv2
import mediapipe as mp
import numpy as np
from keras import backend as K
from tensorflow.keras.models import load_model
import os

# Detect and initialize the TPU
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU
    print('Running on TPU ', tpu.master())
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
except ValueError:
    print("No TPU detected. Using CPU or GPU instead.")
    strategy = tf.distribute.MirroredStrategy()  # Default to GPU/CPU if TPU is unavailable

# Load the previously saved model within the strategy scope
with strategy.scope():
    loaded_model = load_model('model.h5')
    print("Model loaded successfully!")

# Create output directories for images and results
output_dir = './output_frames'
os.makedirs(output_dir, exist_ok=True)

# Function to extract pose data and save frames with pose landmarks
def extract_pose_and_save_frames(video_path):
    mp_pose = mp.solutions.pose
    pose = mp_pose.Pose()

    frame_counter = 0  # To count frames
    pose_data = []  # To store pose landmarks
    cap = cv2.VideoCapture(video_path)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("End of video or error reading the video.")
            break

        # Convert frame to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Process the frame to extract pose landmarks
        result = pose.process(rgb_frame)

        # Draw landmarks on the frame
        if result.pose_landmarks:
            print(f"Pose landmarks detected for frame {frame_counter}!")
            mp.solutions.drawing_utils.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)

            # Save the frame with landmarks as an image
            frame_path = os.path.join(output_dir, f'frame_{frame_counter}.jpg')
            cv2.imwrite(frame_path, frame)

            # Append pose landmarks to the pose_data list
            landmarks = []
            for landmark in result.pose_landmarks.landmark:
                landmarks.append([landmark.x, landmark.y, landmark.z])
            pose_data.append(landmarks)

        frame_counter += 1

    cap.release()

    if pose_data:  # Check if pose_data is not empty
        return np.array(pose_data)
    else:
        print("No pose data extracted from the video.")
        return np.array([])  # Return empty array if no data found

# Path to your input video
input_video_path = 'V1.mp4'  # Replace with your video path
pose_data = extract_pose_and_save_frames(input_video_path)

# Ensure the pose data is not empty before processing further
if pose_data.size > 0:
    # Normalize and reshape the pose data
    pose_data = pose_data / np.max(pose_data)  # Normalize

    # Check the number of landmarks and features
    num_frames = pose_data.shape[0]
    num_landmarks = pose_data.shape[1] if num_frames > 0 else 0
    features = num_landmarks * 3  # 3 coordinates (x, y, z) for each landmark

    # Average the pose data across all frames (for whole video violence detection)
    pose_data_avg = np.mean(pose_data, axis=0)  # Average over time steps
    pose_data_avg = pose_data_avg.reshape((1, 1, features))  # Reshape to (1, 1, features)

    # Clear the Keras session before making predictions
    K.clear_session()

    # Log and predict violence based on the averaged pose data
    print("Pose data shape for violence prediction:", pose_data_avg.shape)

    print("Normalized pose data:", pose_data_avg)

    # Make a prediction for the entire video
    if pose_data_avg.shape == (1, 1, features):
        try:
            # Make a prediction with the loaded model
            prediction = loaded_model.predict(pose_data_avg)

            # Set the prediction threshold (adjust as necessary)
            prediction_threshold = 0.0012

            # Check if violence is detected in the entire video
            if prediction[0][0] > prediction_threshold:
                violence_result = "Overall violence detected in the video."
            else:
                violence_result = "No violence detected in the video."
        except Exception as e:
            print("Error during prediction:", e)
    else:
        print("Invalid input shape for violence prediction.")
else:
    print("No pose data available for the video.")

# Define video writer parameters
# Define video writer parameters for MP4 output
# Create the output video with violence detection result
output_video_path = './final_outputV.mp4'  # Save as MP4 instead of AVI

# Reinitialize cap to read from the original video
cap = cv2.VideoCapture(input_video_path)  # Read from the original video

# Define video writer parameters
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
fourcc = cv2.VideoWriter_fourcc(*'XVID')

# Create VideoWriter object
out_with_text = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

# Process frames and add violence detection result
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Add text to the frame to indicate if violence was detected
    font = cv2.FONT_HERSHEY_SIMPLEX
    caption = violence_result
    cv2.putText(frame, violence_result, (50, 50), font, 1, (0, 255, 0), 2, cv2.LINE_AA)

    # Write the frame with text to the final output video
    out_with_text.write(frame)

cap.release()
out_with_text.release()

print(f"Final video with violence detection result saved as '{output_video_path}'")

